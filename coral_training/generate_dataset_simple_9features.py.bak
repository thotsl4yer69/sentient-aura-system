#!/usr/bin/env python3
"""
Coral TPU Training Dataset Generator

Generates visualization training examples by asking the LLM to describe
how to arrange particles for different states and sensor configurations.

This creates a dataset that the Coral TPU can learn from to generate
real-time particle visualizations at 60 FPS.
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import numpy as np
from pathlib import Path
import logging
from typing import Dict, List, Tuple
import time

from sentient_aura.api_manager import APIManager
from world_state import WorldState

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DatasetGenerator:
    """Generate training dataset for Coral TPU particle visualization."""

    def __init__(self, num_particles=10000):
        """
        Initialize dataset generator.

        Args:
            num_particles: Number of particles to generate (reduced for training speed)
        """
        self.num_particles = num_particles
        self.world_state = WorldState()
        self.api_manager = APIManager(world_state=self.world_state)

        # Training dataset storage
        self.dataset_dir = Path(__file__).parent / 'datasets'
        self.dataset_dir.mkdir(exist_ok=True)

        # States to generate examples for
        self.states = ['idle', 'listening', 'processing', 'speaking', 'executing']

        # Sensor configurations to test
        self.sensor_configs = [
            {'temperature': 22.0, 'humidity': 45.0, 'motion': False, 'audio_level': 0.0},
            {'temperature': 25.0, 'humidity': 50.0, 'motion': True, 'audio_level': 0.3},
            {'temperature': 20.0, 'humidity': 40.0, 'motion': False, 'audio_level': 0.7},
            {'temperature': 23.0, 'humidity': 48.0, 'motion': True, 'audio_level': 0.5},
        ]

    def generate_visualization_prompt(self, state: str, sensors: Dict) -> str:
        """Generate LLM prompt for visualization."""
        prompt = f"""You are designing a particle-based visualization for a sentient AI.

Current state: {state}
Sensor data: {json.dumps(sensors, indent=2)}

Describe how to arrange {self.num_particles} particles in 3D space to represent this state.

Be specific about:
1. Core structure (shape, percentage of particles)
2. Movement patterns
3. Response to sensor data
4. Particle distribution

Format your response as structured instructions:
- CORE: <percentage>% particles, <shape description>
- AURA: <percentage>% particles, <pattern description>
- FLOW: <percentage>% particles, <movement description>
- SENSORS: How to incorporate sensor data

Keep it concise and mathematical."""

        return prompt

    def parse_llm_response_to_particles(self, state: str, sensors: Dict, llm_response: str) -> np.ndarray:
        """
        Parse LLM description into actual particle positions.

        Args:
            state: Current state
            sensors: Sensor data
            llm_response: LLM's description

        Returns:
            np.ndarray: Particle positions (N, 3)
        """
        positions = np.zeros((self.num_particles, 3), dtype=np.float32)

        # Simple rule-based parser for now
        # In production, this could be more sophisticated

        # Extract percentages (rough parsing)
        import re
        percentages = re.findall(r'(\d+)%', llm_response)

        if len(percentages) >= 3:
            core_pct = float(percentages[0]) / 100.0
            aura_pct = float(percentages[1]) / 100.0
        else:
            core_pct = 0.4
            aura_pct = 0.3

        flow_pct = 1.0 - core_pct - aura_pct

        # Generate particle positions based on parsed structure
        n_core = int(self.num_particles * core_pct)
        n_aura = int(self.num_particles * aura_pct)
        n_flow = self.num_particles - n_core - n_aura

        idx = 0

        # Core particles - sphere
        for i in range(n_core):
            theta = np.random.uniform(0, 2 * np.pi)
            phi = np.random.uniform(0, np.pi)
            r = np.random.uniform(0, 0.3)

            x = r * np.sin(phi) * np.cos(theta)
            y = r * np.sin(phi) * np.sin(theta) + 1.0  # Centered at y=1.0
            z = r * np.cos(phi)

            positions[idx] = [x, y, z]
            idx += 1

        # Aura particles - wider sphere
        for i in range(n_aura):
            theta = np.random.uniform(0, 2 * np.pi)
            phi = np.random.uniform(0, np.pi)
            r = np.random.uniform(0.3, 0.6)

            x = r * np.sin(phi) * np.cos(theta)
            y = r * np.sin(phi) * np.sin(theta) + 1.0
            z = r * np.cos(phi)

            positions[idx] = [x, y, z]
            idx += 1

        # Flow particles - scattered
        for i in range(n_flow):
            x = np.random.uniform(-1.0, 1.0)
            y = np.random.uniform(0.0, 2.0)
            z = np.random.uniform(-1.0, 1.0)

            positions[idx] = [x, y, z]
            idx += 1

        # Modulate based on sensor data
        if sensors.get('motion'):
            # Shift particles toward motion direction
            positions[:, 0] += 0.1

        if sensors.get('audio_level', 0) > 0.5:
            # Add vertical spread for high audio
            positions[:, 1] += np.random.uniform(-0.2, 0.2, size=self.num_particles)

        return positions

    def encode_state_sensors(self, state: str, sensors: Dict) -> np.ndarray:
        """
        Encode state and sensors as input tensor for model.

        Returns:
            np.ndarray: Input tensor (1, N) where N is total features
        """
        # State one-hot encoding (5 states)
        state_encoding = np.zeros(5, dtype=np.float32)
        state_idx = self.states.index(state)
        state_encoding[state_idx] = 1.0

        # Sensor encoding (4 values)
        sensor_encoding = np.array([
            sensors.get('temperature', 20.0) / 30.0,  # Normalize 0-30Â°C
            sensors.get('humidity', 50.0) / 100.0,     # Normalize 0-100%
            1.0 if sensors.get('motion', False) else 0.0,
            sensors.get('audio_level', 0.0),           # Already 0-1
        ], dtype=np.float32)

        # Combine
        input_tensor = np.concatenate([state_encoding, sensor_encoding])
        return input_tensor.reshape(1, -1)

    def generate_example(self, state: str, sensors: Dict) -> Dict:
        """Generate single training example."""
        logger.info(f"Generating example: state={state}, sensors={sensors}")

        # Generate visualization prompt
        prompt = self.generate_visualization_prompt(state, sensors)

        # Get LLM response
        response = self.api_manager.chat(prompt, stream=False)

        if not response.is_success():
            logger.error(f"LLM failed: {response.error}")
            return None

        llm_description = response.content
        logger.info(f"LLM description: {llm_description[:200]}...")

        # Parse to particle positions
        particle_positions = self.parse_llm_response_to_particles(state, sensors, llm_description)

        # Encode input
        input_tensor = self.encode_state_sensors(state, sensors)

        # Create example
        example = {
            'state': state,
            'sensors': sensors,
            'llm_description': llm_description,
            'input_tensor': input_tensor.tolist(),
            'particle_positions': particle_positions.tolist(),
        }

        return example

    def generate_dataset(self, num_examples_per_config: int = 5) -> List[Dict]:
        """
        Generate complete training dataset.

        Args:
            num_examples_per_config: Examples per state/sensor combination

        Returns:
            List of training examples
        """
        dataset = []
        total = len(self.states) * len(self.sensor_configs) * num_examples_per_config

        logger.info(f"Generating {total} training examples...")
        logger.info(f"States: {self.states}")
        logger.info(f"Sensor configs: {len(self.sensor_configs)}")

        count = 0
        for state in self.states:
            for sensors in self.sensor_configs:
                for i in range(num_examples_per_config):
                    count += 1
                    logger.info(f"[{count}/{total}] Generating example...")

                    example = self.generate_example(state, sensors)
                    if example:
                        dataset.append(example)

                    # Rate limit to avoid overwhelming LLM
                    time.sleep(2)

        logger.info(f"Generated {len(dataset)} examples")
        return dataset

    def save_dataset(self, dataset: List[Dict], filename: str = 'training_dataset.json'):
        """Save dataset to disk."""
        output_path = self.dataset_dir / filename

        with open(output_path, 'w') as f:
            json.dump(dataset, f, indent=2)

        logger.info(f"Saved dataset to {output_path}")
        logger.info(f"Dataset size: {len(dataset)} examples")

        # Save summary
        summary = {
            'num_examples': len(dataset),
            'num_particles': self.num_particles,
            'states': self.states,
            'sensor_configs': self.sensor_configs,
        }

        summary_path = self.dataset_dir / 'dataset_summary.json'
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2)

        logger.info(f"Saved summary to {summary_path}")

def main():
    """Generate training dataset."""
    logger.info("=" * 70)
    logger.info("Coral TPU Training Dataset Generator")
    logger.info("=" * 70)

    # Create generator (use fewer particles for faster training)
    generator = DatasetGenerator(num_particles=10000)

    # Generate dataset (Option B: 20 examples for faster iteration)
    logger.info("Starting dataset generation (this will take a while)...")
    dataset = generator.generate_dataset(num_examples_per_config=1)  # 1 example per config = 20 total

    # Save dataset
    generator.save_dataset(dataset)

    logger.info("=" * 70)
    logger.info("Dataset generation complete!")
    logger.info("=" * 70)
    logger.info("Next steps:")
    logger.info("1. Review dataset: less coral_training/datasets/training_dataset.json")
    logger.info("2. Train model: python3 coral_training/train_model.py")
    logger.info("=" * 70)

if __name__ == '__main__':
    main()
