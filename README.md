# Sentient Aura System ğŸ¤–

**Advanced AI companion with voice control, 500K particle visualization, and real-time sensor fusion**

[![Python 3.9](https://img.shields.io/badge/python-3.9-blue.svg)](https://www.python.org/downloads/)
[![Coral TPU](https://img.shields.io/badge/Coral-TPU-green.svg)](https://coral.ai/)
[![Three.js](https://img.shields.io/badge/Three.js-WebGL-red.svg)](https://threejs.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸŒŸ Features

### Voice Interface
- **Speech Recognition:** Vosk offline STT (no internet required)
- **Text-to-Speech:** Piper neural TTS (natural voice)
- **Wake Word Detection:** Porcupine ("computer", "jarvis")
- **Continuous Listening:** 15-second command window

### Advanced 3D Visualization
- **500,000 Particles** with WebGL custom shaders
- **5 Visualization Modes:**
  - ğŸ§ **HUMANOID** - AI avatar with face, torso, and energy aura
  - ğŸ—ºï¸ **SPATIAL** - 3D room mapping with detected objects
  - ğŸ“¡ **RF_SPECTRUM** - Radio frequency visualization (Flipper Zero)
  - ğŸ§  **NEURAL_NETWORK** - AI processing visualization
  - âš™ï¸ **ACTION_SPACE** - Tool interaction display

### Real-Time Sensor Integration
- Temperature/humidity (affects particle movement and color)
- Motion detection (triggers turbulence)
- Audio level (creates wave effects)
- Object detection (3D spatial positioning)
- RF signals (frequency layering visualization)

### Hardware Support
- **Google Coral Edge TPU** (4 TOPS, 60 FPS inference)
- **Flipper Zero** (RF scanning and jamming)
- **BNO055 9-DOF IMU** (orientation sensing)
- **LoRaWAN Radio** (long-range communication)
- **WiFi/Bluetooth** (network scanning)
- **Cameras** (vision with object detection)

## ğŸš€ Quick Start

### Prerequisites
- Raspberry Pi 4/5 or similar ARM64 device
- Python 3.9 (for Coral TPU compatibility)
- Optional: Google Coral USB Accelerator
- Optional: Microphone for voice input

### Installation

```bash
# Clone repository
git clone https://github.com/thotsl4yer69/sentient-aura-system.git
cd sentient-aura-system

# Install dependencies
chmod +x install_dependencies.sh
./install_dependencies.sh

# Or manually:
pip install -r requirements.txt
```

### Running the System

```bash
# Full system with voice and GUI
python3 sentient_aura_main.py

# Headless mode (no GUI)
python3 sentient_aura_main.py --headless

# Test mode (no voice, simulated commands)
python3 sentient_aura_main.py --test

# With supervisor (auto-restart on crash)
python3 supervisor.py
```

### Test Hardware Detection

```bash
# Run diagnostic to see detected hardware
python3 quick_diagnostic.py
```

### Test Visualization Data Flow

```bash
# Start visualization test (broadcasts simulated sensor data)
python3 test_visualization_flow.py

# Then open in browser:
# file:///path/to/sentient-aura-system/sentient_aura/sentient_core.html
```

## ğŸ“Š System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hardware    â”‚  (Coral TPU, Flipper, sensors, cameras)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Daemons    â”‚  (WiFi, Bluetooth, Vision, IMU, Hardware Monitor)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WorldState  â”‚  (Centralized state management)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SentientCore â”‚  (Brain - intent parsing, action execution)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WebSocket   â”‚  (Port 8765, JSON + Binary protocol)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Three.js    â”‚  (500K particle visualization)
â”‚   WebGL GUI  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¨ Visualization Modes

### HUMANOID Mode (Orange)
AI avatar with anatomically-inspired particle distribution:
- **Head:** 35% (175,000 particles with facial features)
- **Torso:** 30% (shoulders and body structure)
- **Aura:** 20% (energy field - orbital/radial/flowing)
- **Flow:** 15% (ambient particles)

The AI can define its own appearance via LLM prompts!

### SPATIAL Mode (Blue)
3D room mapping showing:
- Camera FOV cone
- Detected objects with 3D positioning
- Depth estimation from 2D bounding boxes
- Real-time object tracking

### RF_SPECTRUM Mode (Red)
Radio frequency visualization:
- Horizontal layers for each frequency
- Signal strength = particle radius
- Real Flipper Zero RF scan data
- Protocol identification

### NEURAL_NETWORK Mode (Green)
AI processing visualization:
- 5-layer neural network structure
- 20 nodes per layer
- Inter-layer connection particles
- Shows active reasoning pathways

### ACTION_SPACE Mode (Purple)
Tool interaction visualization:
- Particles orbit action targets
- Shows what the AI is manipulating
- Real-time tool engagement feedback

## ğŸ”§ Configuration

Edit `sentient_aura/config.py`:

```python
# Wake word detection
PORCUPINE_ACCESS_KEY = "your-key"  # Get from picovoice.ai
WAKE_WORDS = ['computer', 'jarvis']
USE_WAKE_WORD = True

# Coral TPU visualization
CORAL_VIZ_ENABLED = True
CORAL_VIZ_TARGET_FPS = 60
CORAL_VIZ_MODEL_PATH = "models/sentient_viz_enhanced_edgetpu.tflite"

# WebSocket server
WEBSOCKET_HOST = "localhost"
WEBSOCKET_PORT = 8765
```

## ğŸ“¡ Binary Protocol

For maximum performance, the system supports binary particle data transmission:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      HEADER (64 bytes)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ version, frameId, fps,          â”‚
â”‚ inferenceMs, particleCount      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   PAYLOAD (120 KB)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Float32Array[500000 * 3]        â”‚
â”‚ [x,y,z, x,y,z, ...]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Performance:** 120 KB binary vs 500 KB+ JSON (4Ã— bandwidth savings)

## ğŸ› Bug Fixes

### Recent Fixes (2025-10-25)
- âœ… Fixed Bluetooth scanner timeout (45s â†’ 2s fast-fail)
- âœ… Fixed HardwareCapability attribute access in diagnostic
- âœ… Installed missing IMU dependencies
- âœ… All diagnostic tests now passing

## ğŸ“ˆ Performance Metrics

- **Startup Time:** 5-10 seconds
- **Visualization FPS:** Target 60 FPS
- **Sensor Updates:** Every 2 seconds via WebSocket
- **Memory Usage:** ~94 MB
- **CPU Usage:** ~15% idle, ~40% active
- **Coral Inference:** <16ms per frame

## ğŸ¯ Voice Commands

```
"Core, show me your sensors"       # Display hardware status
"Core, how are you doing"           # System health check
"Core, scan for frequencies"        # RF spectrum scan
"Core, any threats detected"        # Check for drone signals
"Core, what's the temperature"      # Read environment sensors
```

## ğŸ“ Documentation

- **CURRENT_STATUS.md** - Complete system status and health
- **VISUALIZATION_STATUS.md** - Full visualization architecture
- **CORAL_TPU_ARCHITECTURE.md** - Coral TPU integration details
- **API_INTEGRATION_SUMMARY.md** - External API integrations
- **DEPLOYMENT_GUIDE.md** - Production deployment guide

## ğŸ”¬ Testing

```bash
# Run all diagnostic tests
python3 quick_diagnostic.py

# Test visualization data flow
python3 test_visualization_flow.py

# Test voice-to-hardware pipeline
python3 test_voice_to_hardware.py

# Test Coral TPU inference
python3 coral_training/test_enhanced_edgetpu.py
```

## ğŸ¤ Contributing

This is a personal research project, but suggestions and feedback are welcome!

## ğŸ“œ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- **Vosk** - Offline speech recognition
- **Piper** - Neural text-to-speech
- **Three.js** - WebGL 3D visualization
- **Google Coral** - Edge TPU acceleration
- **Porcupine** - Wake word detection

## ğŸ”® Future Roadmap

- [ ] LLM integration (Ollama for advanced reasoning)
- [ ] Multi-sensor fusion for threat detection
- [ ] Autonomous behavior learning
- [ ] Home Assistant integration
- [ ] Mobile app for remote control
- [ ] AR/VR visualization mode

## ğŸ“ Contact

GitHub: [@thotsl4yer69](https://github.com/thotsl4yer69)

---

**Built with â¤ï¸ for the future of human-AI interaction**

*The AI doesn't just tell you about the world - it SHOWS you how it perceives reality through 500,000 dancing particles.*
