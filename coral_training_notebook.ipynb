{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentient Core: Coral TPU Pixel Controller Training\n",
    "\n",
    "This notebook trains a TensorFlow model to control 500,000 particle positions based on sensor data.\n",
    "\n",
    "**Training Pipeline:**\n",
    "1. Generate synthetic sensor → particle behavior dataset\n",
    "2. Train quantization-aware model\n",
    "3. Convert to TensorFlow Lite with full int8 quantization\n",
    "4. Download `.tflite` model for Edge TPU compilation\n",
    "\n",
    "**Hardware Target:** Google Coral Edge TPU (4 TOPS, USB Accelerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q tensorflow==2.13.0\n",
    "!pip install -q tensorflow-model-optimization\n",
    "!pip install -q matplotlib seaborn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Generation: Synthetic Sensor Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sensor_data(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Generate synthetic sensor readings covering full operational range.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: (n_samples, 22) normalized sensor features\n",
    "    \"\"\"\n",
    "    data = np.zeros((n_samples, 22), dtype=np.float32)\n",
    "    \n",
    "    # Environment (8 features) - indices 0-7\n",
    "    data[:, 0] = np.random.uniform(10, 40, n_samples) / 50.0  # temperature (°C) / 50\n",
    "    data[:, 1] = np.random.uniform(20, 80, n_samples) / 100.0  # humidity (%) / 100\n",
    "    data[:, 2] = np.random.uniform(980, 1040, n_samples) / 1100.0  # pressure (hPa) / 1100\n",
    "    data[:, 3] = np.random.uniform(10000, 100000, n_samples) / 200000.0  # gas_resistance (Ω) / 200k\n",
    "    data[:, 4] = np.random.uniform(0, 500, n_samples) / 1000.0  # oxidising (ppm) / 1000\n",
    "    data[:, 5] = np.random.uniform(0, 500, n_samples) / 1000.0  # reducing (ppm) / 1000\n",
    "    data[:, 6] = np.random.uniform(0, 100, n_samples) / 200.0  # nh3 (ppm) / 200\n",
    "    data[:, 7] = np.random.uniform(0, 1000, n_samples) / 1000.0  # light_level (lux) / 1000\n",
    "    \n",
    "    # Audio (2 features) - indices 8-9\n",
    "    data[:, 8] = np.random.uniform(30, 80, n_samples) / 100.0  # ambient_noise (dB) / 100\n",
    "    data[:, 9] = np.random.uniform(0, 360, n_samples) / 360.0  # sound_direction (degrees) / 360\n",
    "    \n",
    "    # Vision (3 features) - indices 10-12\n",
    "    data[:, 10] = np.random.choice([0.0, 1.0], n_samples)  # motion_detected (bool)\n",
    "    data[:, 11] = np.random.randint(0, 10, n_samples) / 10.0  # detected_objects_count / 10\n",
    "    data[:, 12] = np.random.randint(0, 5, n_samples) / 5.0  # faces_detected_count / 5\n",
    "    \n",
    "    # Location (3 features) - indices 13-15 (using relative values)\n",
    "    data[:, 13] = np.random.uniform(-90, 90, n_samples) / 180.0 + 0.5  # latitude / 180 + 0.5\n",
    "    data[:, 14] = np.random.uniform(-180, 180, n_samples) / 360.0 + 0.5  # longitude / 360 + 0.5\n",
    "    data[:, 15] = np.random.uniform(0, 500, n_samples) / 1000.0  # altitude (m) / 1000\n",
    "    \n",
    "    # Power (3 features) - indices 16-18\n",
    "    data[:, 16] = np.random.uniform(20, 100, n_samples) / 100.0  # battery_charge (%) / 100\n",
    "    data[:, 17] = np.random.uniform(3.3, 4.2, n_samples) / 5.0  # battery_voltage (V) / 5\n",
    "    data[:, 18] = np.random.choice([0.0, 1.0], n_samples)  # is_charging (bool)\n",
    "    \n",
    "    # System (3 features) - indices 19-21\n",
    "    data[:, 19] = np.random.uniform(0, 86400, n_samples) / 86400.0  # uptime (seconds) / 24h\n",
    "    data[:, 20] = np.random.randint(1, 10, n_samples) / 10.0  # active_daemons_count / 10\n",
    "    data[:, 21] = np.random.uniform(40, 70, n_samples) / 100.0  # cpu_temp (°C) / 100\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating sensor data...\")\n",
    "X = generate_sensor_data(10000)\n",
    "print(f\"Generated {X.shape[0]} samples with {X.shape[1]} features\")\n",
    "print(f\"Feature range: [{X.min():.3f}, {X.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Expert Labeling: Heuristic Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_label_particle_params(sensor_data):\n",
    "    \"\"\"\n",
    "    Apply expert rules to generate ideal particle behavior parameters.\n",
    "    \n",
    "    Args:\n",
    "        sensor_data: (n_samples, 22) normalized sensor features\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: (n_samples, 12) particle behavior parameters\n",
    "    \"\"\"\n",
    "    n_samples = sensor_data.shape[0]\n",
    "    params = np.zeros((n_samples, 12), dtype=np.float32)\n",
    "    \n",
    "    # Extract key sensors (denormalized for rule logic)\n",
    "    temperature = sensor_data[:, 0] * 50.0  # 0-50°C\n",
    "    humidity = sensor_data[:, 1] * 100.0  # 0-100%\n",
    "    light_level = sensor_data[:, 7] * 1000.0  # 0-1000 lux\n",
    "    ambient_noise = sensor_data[:, 8] * 100.0  # 0-100 dB\n",
    "    motion_detected = sensor_data[:, 10]  # 0 or 1\n",
    "    detected_objects = sensor_data[:, 11] * 10.0  # 0-10 objects\n",
    "    battery_charge = sensor_data[:, 16] * 100.0  # 0-100%\n",
    "    is_charging = sensor_data[:, 18]  # 0 or 1\n",
    "    \n",
    "    # Rule 1: Temperature affects color hue (blue=cold, red=hot)\n",
    "    params[:, 3] = np.clip((temperature - 15) / 30.0, 0, 1)  # color_hue_shift\n",
    "    \n",
    "    # Rule 2: Humidity affects cohesion (high humidity = tight clusters)\n",
    "    params[:, 0] = np.clip(humidity / 100.0, 0, 1)  # swarm_cohesion\n",
    "    \n",
    "    # Rule 3: Motion triggers turbulence and speed\n",
    "    params[:, 2] = motion_detected * 0.8 + np.random.uniform(0, 0.2, n_samples)  # turbulence\n",
    "    params[:, 1] = motion_detected * 0.7 + detected_objects * 0.03  # flow_speed\n",
    "    \n",
    "    # Rule 4: Light level controls brightness\n",
    "    params[:, 4] = np.clip(light_level / 1000.0, 0.2, 1.0)  # brightness (min 0.2)\n",
    "    \n",
    "    # Rule 5: Ambient noise drives pulse frequency\n",
    "    params[:, 5] = np.clip((ambient_noise - 30) / 50.0, 0, 1)  # pulse_frequency\n",
    "    \n",
    "    # Rule 6: Low battery = reduced particle size and glow\n",
    "    params[:, 10] = np.clip(battery_charge / 100.0, 0.3, 1.0)  # particle_size\n",
    "    params[:, 11] = np.clip(battery_charge / 100.0, 0.2, 1.0)  # glow_intensity\n",
    "    \n",
    "    # Rule 7: Charging state affects vertical bias (rising energy)\n",
    "    params[:, 7] = is_charging * 0.5 - 0.25  # vertical_bias (-0.25 to +0.25)\n",
    "    \n",
    "    # Rule 8: Default symmetry based on temperature variance\n",
    "    params[:, 6] = 0.5 + np.random.uniform(-0.2, 0.2, n_samples)  # symmetry\n",
    "    \n",
    "    # Rule 9: Horizontal spread inversely related to cohesion\n",
    "    params[:, 8] = 1.0 - params[:, 0] * 0.5  # horizontal_spread\n",
    "    \n",
    "    # Rule 10: Depth layering based on object count (more objects = more depth)\n",
    "    params[:, 9] = np.clip(detected_objects / 10.0, 0.2, 0.8)  # depth_layering\n",
    "    \n",
    "    # Ensure all values in [0, 1] (except vertical_bias in [-1, 1])\n",
    "    params[:, :7] = np.clip(params[:, :7], 0, 1)\n",
    "    params[:, 7] = np.clip(params[:, 7], -1, 1)  # vertical_bias\n",
    "    params[:, 8:] = np.clip(params[:, 8:], 0, 1)\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Generate labels\n",
    "print(\"Applying expert rules to generate labels...\")\n",
    "y = expert_label_particle_params(X)\n",
    "print(f\"Generated {y.shape[0]} labels with {y.shape[1]} parameters\")\n",
    "print(f\"Label range: [{y.min():.3f}, {y.max():.3f}]\")\n",
    "\n",
    "# Visualize label distributions\n",
    "param_names = [\n",
    "    'swarm_cohesion', 'flow_speed', 'turbulence', 'color_hue_shift',\n",
    "    'brightness', 'pulse_frequency', 'symmetry', 'vertical_bias',\n",
    "    'horizontal_spread', 'depth_layering', 'particle_size', 'glow_intensity'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.hist(y[:, i], bins=50, alpha=0.7, edgecolor='black')\n",
    "    ax.set_title(param_names[i])\n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset: 70% train, 20% validation, 10% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture: Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pixel_controller_model():\n",
    "    \"\"\"\n",
    "    Create compact dense neural network for Coral Edge TPU.\n",
    "    \n",
    "    Input: 22 sensor features\n",
    "    Output: 12 particle behavior parameters\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(22,), name='sensor_input'),\n",
    "        \n",
    "        # Hidden layers with decreasing size\n",
    "        tf.keras.layers.Dense(128, activation='relu', name='dense_128'),\n",
    "        tf.keras.layers.Dropout(0.2, name='dropout_1'),\n",
    "        \n",
    "        tf.keras.layers.Dense(64, activation='relu', name='dense_64'),\n",
    "        tf.keras.layers.Dropout(0.15, name='dropout_2'),\n",
    "        \n",
    "        tf.keras.layers.Dense(32, activation='relu', name='dense_32'),\n",
    "        \n",
    "        # Output layer: 12 parameters\n",
    "        # Using sigmoid for 11 params (0-1 range) and tanh for vertical_bias (-1 to 1)\n",
    "        tf.keras.layers.Dense(12, activation='sigmoid', name='particle_params')\n",
    "    ], name='SentientPixelController')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile model\n",
    "base_model = create_pixel_controller_model()\n",
    "base_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "base_model.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "total_params = base_model.count_params()\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Estimated model size (float32): {total_params * 4 / 1024:.2f} KB\")\n",
    "print(f\"Estimated model size (int8): {total_params / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base model for initial convergence\n",
    "print(\"Training base model...\")\n",
    "\n",
    "history = base_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='Train Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Training & Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(history.history['mae'], label='Train MAE')\n",
    "ax2.plot(history.history['val_mae'], label='Val MAE')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean Absolute Error')\n",
    "ax2.set_title('Training & Validation MAE')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae, test_mse = base_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  MSE Loss: {test_mse:.6f}\")\n",
    "print(f\"  MAE: {test_mae:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Quantization-Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantization-aware training\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "q_aware_model = quantize_model(base_model)\n",
    "\n",
    "q_aware_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower LR for fine-tuning\n",
    "    loss='mse',\n",
    "    metrics=['mae', 'mse']\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning with quantization-aware training...\")\n",
    "q_history = q_aware_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate quantized model\n",
    "q_test_loss, q_test_mae, q_test_mse = q_aware_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"\\nQuantized Model Test Performance:\")\n",
    "print(f\"  MSE Loss: {q_test_mse:.6f}\")\n",
    "print(f\"  MAE: {q_test_mae:.6f}\")\n",
    "print(f\"\\nAccuracy degradation from quantization: {(q_test_mae - test_mae) / test_mae * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convert to TensorFlow Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representative dataset for full integer quantization\n",
    "def representative_dataset():\n",
    "    \"\"\"\n",
    "    Provide representative samples for post-training quantization calibration.\n",
    "    \"\"\"\n",
    "    for i in range(100):\n",
    "        # Use actual training samples\n",
    "        sample = X_train[i:i+1].astype(np.float32)\n",
    "        yield [sample]\n",
    "\n",
    "# Configure TFLite converter for Edge TPU\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "\n",
    "# Enable full integer quantization (REQUIRED for Edge TPU)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert model\n",
    "print(\"Converting to TensorFlow Lite (int8)...\")\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save model\n",
    "tflite_filename = 'sentient_pixel_controller.tflite'\n",
    "with open(tflite_filename, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"\\nTFLite model saved: {tflite_filename}\")\n",
    "print(f\"Model size: {len(tflite_model) / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test TFLite Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model and test\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_filename)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"TFLite Model Details:\")\n",
    "print(f\"Input shape: {input_details[0]['shape']}\")\n",
    "print(f\"Input dtype: {input_details[0]['dtype']}\")\n",
    "print(f\"Input quantization: {input_details[0]['quantization']}\")\n",
    "print(f\"\\nOutput shape: {output_details[0]['shape']}\")\n",
    "print(f\"Output dtype: {output_details[0]['dtype']}\")\n",
    "print(f\"Output quantization: {output_details[0]['quantization']}\")\n",
    "\n",
    "# Test inference on sample\n",
    "def run_tflite_inference(interpreter, input_data):\n",
    "    \"\"\"Run inference on TFLite model with quantization.\"\"\"\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Quantize input\n",
    "    input_scale, input_zero_point = input_details[0]['quantization']\n",
    "    input_data_int8 = (input_data / input_scale + input_zero_point).astype(np.int8)\n",
    "    \n",
    "    # Run inference\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data_int8)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # Dequantize output\n",
    "    output_data_int8 = interpreter.get_tensor(output_details[0]['index'])\n",
    "    output_scale, output_zero_point = output_details[0]['quantization']\n",
    "    output_data = (output_data_int8.astype(np.float32) - output_zero_point) * output_scale\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "# Compare TFLite vs original model\n",
    "test_sample = X_test[:5]\n",
    "tflite_predictions = np.array([run_tflite_inference(interpreter, sample.reshape(1, -1)) for sample in test_sample])\n",
    "original_predictions = q_aware_model.predict(test_sample, verbose=0)\n",
    "\n",
    "print(\"\\nPrediction Comparison (first 5 test samples):\")\n",
    "print(f\"Original model shape: {original_predictions.shape}\")\n",
    "print(f\"TFLite model shape: {tflite_predictions.shape}\")\n",
    "print(f\"Mean absolute difference: {np.mean(np.abs(original_predictions - tflite_predictions.squeeze())):.6f}\")\n",
    "print(f\"Max absolute difference: {np.max(np.abs(original_predictions - tflite_predictions.squeeze())):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Training Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata for deployment\n",
    "metadata = {\n",
    "    \"model_name\": \"SentientPixelController\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"training_date\": \"2025-10-26\",\n",
    "    \"input_features\": 22,\n",
    "    \"output_params\": 12,\n",
    "    \"total_params\": int(total_params),\n",
    "    \"model_size_kb\": len(tflite_model) / 1024,\n",
    "    \"test_mae\": float(q_test_mae),\n",
    "    \"test_mse\": float(q_test_mse),\n",
    "    \"input_normalization\": {\n",
    "        \"temperature\": \"/ 50.0\",\n",
    "        \"humidity\": \"/ 100.0\",\n",
    "        \"pressure\": \"/ 1100.0\",\n",
    "        \"light_level\": \"/ 1000.0\",\n",
    "        \"battery_charge\": \"/ 100.0\"\n",
    "    },\n",
    "    \"output_params\": [\n",
    "        \"swarm_cohesion\", \"flow_speed\", \"turbulence\", \"color_hue_shift\",\n",
    "        \"brightness\", \"pulse_frequency\", \"symmetry\", \"vertical_bias\",\n",
    "        \"horizontal_spread\", \"depth_layering\", \"particle_size\", \"glow_intensity\"\n",
    "    ],\n",
    "    \"quantization\": {\n",
    "        \"input_scale\": float(input_details[0]['quantization'][0]),\n",
    "        \"input_zero_point\": int(input_details[0]['quantization'][1]),\n",
    "        \"output_scale\": float(output_details[0]['quantization'][0]),\n",
    "        \"output_zero_point\": int(output_details[0]['quantization'][1])\n",
    "    },\n",
    "    \"next_steps\": [\n",
    "        \"1. Download sentient_pixel_controller.tflite\",\n",
    "        \"2. Transfer to Raspberry Pi\",\n",
    "        \"3. Compile with: edgetpu_compiler sentient_pixel_controller.tflite\",\n",
    "        \"4. Integrate into sentient_core.py using coral_pixel_engine.py\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('model_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadata saved to model_metadata.json\")\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TFLite model and metadata\n",
    "print(\"Downloading files...\")\n",
    "files.download(tflite_filename)\n",
    "files.download('model_metadata.json')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nNext steps on Raspberry Pi:\")\n",
    "print(\"1. Install Edge TPU compiler:\")\n",
    "print(\"   curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\")\n",
    "print(\"   echo 'deb https://packages.cloud.google.com/apt coral-edgetpu-stable main' | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\")\n",
    "print(\"   sudo apt-get update && sudo apt-get install edgetpu-compiler\")\n",
    "print(\"\\n2. Compile for Edge TPU:\")\n",
    "print(\"   edgetpu_compiler sentient_pixel_controller.tflite\")\n",
    "print(\"\\n3. Move compiled model:\")\n",
    "print(\"   mkdir -p ~/Sentient-Core-v4/models\")\n",
    "print(\"   mv sentient_pixel_controller_edgetpu.tflite ~/Sentient-Core-v4/models/\")\n",
    "print(\"\\n4. Integrate coral_pixel_engine.py into sentient_core.py\")\n",
    "print(\"\\n5. Launch and watch the pixels come ALIVE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
