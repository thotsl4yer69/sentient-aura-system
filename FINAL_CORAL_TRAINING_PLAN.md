# FINAL CORAL TPU TRAINING PLAN - Sentient Cortana Pixels

**Mission:** Train a Google Coral Edge TPU model to intelligently control 500,000 particles representing Cortana's holographic form, driven by real-time sensor data.

**Target Hardware:** Google Coral USB Accelerator (4 TOPS, 2W)
**Training Platform:** Google Colab (free GPU)
**Deployment:** Raspberry Pi 500+ with Coral TPU

---

## Table of Contents

1. [System Overview](#system-overview)
2. [Model Architecture](#model-architecture)
3. [Input Specification (22 Sensors)](#input-specification)
4. [Output Specification (12 Cortana Behaviors)](#output-specification)
5. [Training Data Generation](#training-data-generation)
6. [Model Training Process](#model-training-process)
7. [Quantization & Compilation](#quantization--compilation)
8. [Integration & Deployment](#integration--deployment)
9. [Testing & Validation](#testing--validation)
10. [Next Steps](#next-steps)

---

## System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SENSORS (22)   â”‚  Temperature, humidity, WiFi, BT, motion, audio, etc.
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  WORLD STATE    â”‚  Centralized sensor data storage
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CORAL TPU INFERENCE (<5ms)     â”‚
â”‚  Model: 22 inputs â†’ 12 outputs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CORTANA PARTICLE BEHAVIORS     â”‚
â”‚  - Cohesion, turbulence, color  â”‚
â”‚  - Breathing, glow, mood        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  500K PARTICLES (WebGL)         â”‚
â”‚  Holo graphic Cortana form      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Model Architecture

### Optimized for Edge TPU: 64â†’64â†’12

```python
import tensorflow as tf

def create_cortana_pixel_model():
    """
    Optimized Dense network for Google Coral Edge TPU.

    Architecture chosen for:
    - Maximum speed (fewer layers = less sequential processing)
    - 100% TPU execution (all ops supported)
    - Small model size (~5,500 parameters)
    """

    model = tf.keras.Sequential([
        # Input layer: 22 sensor features
        tf.keras.layers.Input(shape=(22,), name='sensor_input'),

        # Hidden layer 1: 64 neurons
        tf.keras.layers.Dense(64, activation='relu', name='hidden_1'),

        # Hidden layer 2: 64 neurons
        tf.keras.layers.Dense(64, activation='relu', name='hidden_2'),

        # Output layer: 12 Cortana behavior parameters
        tf.keras.layers.Dense(12, activation='sigmoid', name='cortana_behaviors')
    ], name='CortanaPixelController')

    return model
```

**Why this architecture:**
- âœ… All FULLY_CONNECTED ops (supported on TPU)
- âœ… ReLU activations (supported on TPU)
- âœ… Sigmoid output (LOGISTIC op, supported on TPU)
- âœ… No Dropout (removed during inference anyway)
- âœ… No Batch Normalization (causes CPU fallback)
- âœ… Wider, shallower = faster on Edge TPU

**Parameter count:** ~5,500 parameters
**Model size:** ~50KB when quantized to int8
**Inference time:** <3ms (predicted)

---

## Input Specification (22 Sensors)

### Sensor Features (Normalized 0-1)

| Index | Feature | Source | Normalization | Always Available? |
|-------|---------|--------|---------------|-------------------|
| **Environment (8 features)** ||||
| 0 | `temperature` | BME680 or CPU | value / 50.0 | âœ… CPU temp fallback |
| 1 | `humidity` | BME680 | value / 100.0 | âŒ |
| 2 | `pressure` | BME680 | value / 1100.0 | âŒ |
| 3 | `gas_resistance` | BME680 | value / 200000.0 | âŒ |
| 4 | `oxidising` | Environmental Array | value / 1000.0 | âŒ |
| 5 | `reducing` | Environmental Array | value / 1000.0 | âŒ |
| 6 | `nh3` | Environmental Array | value / 200.0 | âŒ |
| 7 | `light_level` | LTR559 | value / 1000.0 | âŒ |
| **Audio (2 features)** ||||
| 8 | `ambient_noise` | Microphone | value / 100.0 | âŒ |
| 9 | `sound_direction` | Mic array | value / 360.0 | âŒ |
| **Vision (3 features)** ||||
| 10 | `motion_detected` | Camera | 0.0 or 1.0 | âŒ |
| 11 | `detected_objects` | Pi AI HAT (future) | count / 10.0 | âŒ |
| 12 | `faces_detected` | Pi AI HAT (future) | count / 5.0 | âŒ |
| **Location (3 features)** ||||
| 13 | `latitude` | GPS | (value / 180.0) + 0.5 | âŒ |
| 14 | `longitude` | GPS | (value / 360.0) + 0.5 | âŒ |
| 15 | `altitude` | GPS | value / 1000.0 | âŒ |
| **Power (3 features)** ||||
| 16 | `battery_charge` | INA219 | value / 100.0 | âŒ |
| 17 | `battery_voltage` | INA219 | value / 5.0 | âŒ |
| 18 | `is_charging` | INA219 | 0.0 or 1.0 | âŒ |
| **System (3 features)** ||||
| 19 | `uptime` | System | value / 86400.0 | âœ… Always |
| 20 | `active_daemons` | System | count / 10.0 | âœ… Always |
| 21 | `cpu_temp` | Thermal | value / 100.0 | âœ… Always |

### Currently Available Sensors (Minimum Viable)

**Raspberry Pi 500+ with NO external hardware:**
- âœ… CPU temperature (always available)
- âœ… System uptime (always available)
- âœ… Active daemon count (always available)
- âœ… WiFi signal strength (Pi built-in WiFi)
- âœ… Bluetooth device count (Pi built-in BT)
- âœ… Time-based patterns (for beautiful fallback)

**With this alone, Cortana will be ALIVE and responsive!**

---

## Output Specification (12 Cortana Behaviors)

### Particle Behavior Parameters

| Index | Parameter | Range | Controls | Visual Effect |
|-------|-----------|-------|----------|---------------|
| 0 | `swarm_cohesion` | 0-1 | Particle clustering | Tight swarm (1.0) â†” Spread out (0.0) |
| 1 | `flow_speed` | 0-1 | Animation velocity | Fast movement (1.0) â†” Slow drift (0.0) |
| 2 | `turbulence` | 0-1 | Chaotic displacement | Stormy (1.0) â†” Calm (0.0) |
| 3 | `color_hue_shift` | 0-1 | Navy â†’ Lavender | Cortana mood color |
| 4 | `brightness` | 0-1 | Overall luminosity | Bright (1.0) â†” Dim (0.0) |
| 5 | `pulse_frequency` | 0-1 | Breathing rate | Fast pulse (1.0) â†” Slow breath (0.0) |
| 6 | `symmetry` | 0-1 | Bilateral mirroring | Perfect mirror (1.0) â†” Asymmetric (0.0) |
| 7 | `vertical_bias` | -1 to 1 | Up/down tendency | Rising (1.0) â†” Falling (-1.0) |
| 8 | `horizontal_spread` | 0-1 | Left-right expansion | Wide (1.0) â†” Narrow (0.0) |
| 9 | `depth_layering` | 0-1 | Z-axis stratification | Deep layers (1.0) â†” Flat (0.0) |
| 10 | `particle_size` | 0-1 | Point size | Large (1.0) â†” Tiny (0.0) |
| 11 | `glow_intensity` | 0-1 | Bloom effect | Glowing (1.0) â†” Matte (0.0) |

---

## Training Data Generation

### Expert Labeling Rules (Cortana-Specific)

```python
import numpy as np
import math
import time

def generate_cortana_training_data(n_samples=10000):
    """
    Generate realistic sensor â†’ Cortana behavior dataset.

    Includes:
    - Sparse sensor scenarios (realistic)
    - Cortana-specific behavior rules
    - Temporal smoothing for natural animations
    """

    X = []  # Sensor inputs (n_samples, 22)
    y = []  # Cortana behaviors (n_samples, 12)

    prev_behaviors = None

    for i in range(n_samples):
        # Generate sensor reading
        sensors = generate_sensor_sample(sparse=True)

        # Apply Cortana expert rules
        behaviors = cortana_expert_rules(sensors, prev_behaviors)

        X.append(sensors)
        y.append(behaviors)

        prev_behaviors = behaviors

    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)


def generate_sensor_sample(sparse=True):
    """Generate single sensor reading with realistic availability."""

    sensors = np.zeros(22, dtype=np.float32)

    if sparse:
        # Simulate realistic sensor availability
        sensor_availability = np.random.choice([
            'minimal',      # 50% (WiFi, BT, system only)
            'environment',  # 20% (+ temp, humidity)
            'audio',        # 15% (+ mic)
            'full',         # 15% (all sensors)
        ], p=[0.5, 0.2, 0.15, 0.15])
    else:
        sensor_availability = 'full'

    # ALWAYS AVAILABLE (system sensors)
    sensors[19] = np.random.uniform(0, 86400) / 86400.0  # uptime
    sensors[20] = np.random.randint(1, 10) / 10.0  # active_daemons
    sensors[21] = (40 + np.random.uniform(0, 30)) / 100.0  # cpu_temp (40-70Â°C)

    # WiFi/BT (Pi built-in)
    wifi_signal = -70 + np.random.uniform(0, 40)  # -70 to -30 dBm
    sensors[13] = (wifi_signal + 100) / 100.0  # Proxy for location/activity

    bt_devices = np.random.randint(0, 5)
    sensors[14] = bt_devices / 10.0  # Proxy for detected_objects

    # TIME-BASED PATTERNS (always available fallback)
    t = time.time() + np.random.uniform(0, 86400)  # Random time
    sensors[0] = (20 + 10 * math.sin(t / 3600 * math.pi / 12)) / 50.0  # Daily temp cycle
    sensors[7] = (500 + 400 * math.sin(t / 3600 * math.pi / 12)) / 1000.0  # Light cycle

    # Additional sensors based on availability
    if sensor_availability in ['environment', 'full']:
        sensors[1] = np.random.uniform(30, 70) / 100.0  # humidity
        sensors[2] = np.random.uniform(980, 1040) / 1100.0  # pressure

    if sensor_availability in ['audio', 'full']:
        sensors[8] = np.random.uniform(30, 80) / 100.0  # ambient_noise
        sensors[9] = np.random.uniform(0, 360) / 360.0  # sound_direction

    if sensor_availability == 'full':
        sensors[3] = np.random.uniform(10000, 100000) / 200000.0  # gas
        sensors[10] = np.random.choice([0.0, 1.0])  # motion
        sensors[11] = np.random.randint(0, 10) / 10.0  # detected_objects
        sensors[12] = np.random.randint(0, 5) / 5.0  # faces
        sensors[16] = np.random.uniform(20, 100) / 100.0  # battery
        sensors[17] = np.random.uniform(3.3, 4.2) / 5.0  # voltage
        sensors[18] = np.random.choice([0.0, 1.0])  # charging

    return sensors


def cortana_expert_rules(sensors, prev_behaviors=None):
    """
    Expert rules for Cortana particle behaviors.

    Based on:
    - Halo lore (Cortana personality traits)
    - Natural holographic appearance
    - Sensor correlations
    """

    behaviors = np.zeros(12, dtype=np.float32)

    # Extract key sensors
    temp = sensors[0] * 50.0  # Denormalize
    humidity = sensors[1] * 100.0
    light = sensors[7] * 1000.0
    audio = sensors[8] * 100.0
    motion = sensors[10]
    cpu_temp = sensors[21] * 100.0
    uptime = sensors[19] * 86400.0

    # === CORTANA-SPECIFIC RULES ===

    # 1. SWARM COHESION - Cortana is graceful, cohesive
    # Base: 0.6 (naturally cohesive)
    # Higher in calm conditions, lower when active
    behaviors[0] = 0.6 + (1.0 - motion) * 0.2 - (audio / 100.0) * 0.3
    behaviors[0] = np.clip(behaviors[0], 0.4, 0.9)

    # 2. FLOW SPEED - Responds to activity
    # Base: 0.4 (moderate)
    # Faster when motion/audio detected
    behaviors[1] = 0.4 + motion * 0.4 + (audio - 40) / 60.0 * 0.3
    behaviors[1] = np.clip(behaviors[1], 0.2, 0.8)

    # 3. TURBULENCE - Cortana shows emotion through particle chaos
    # Calm when idle, turbulent when active or hot
    heat_stress = max(0, (cpu_temp - 60) / 20.0)  # 60-80Â°C
    behaviors[2] = motion * 0.6 + (audio - 40) / 60.0 * 0.4 + heat_stress * 0.3
    behaviors[2] = np.clip(behaviors[2], 0.1, 0.7)

    # 4. COLOR HUE SHIFT - Cortana's mood
    # Navy (0.0) = Calm, Cyan (0.5) = Normal, Lavender (1.0) = Alert/Warm
    # Affected by temperature and activity
    mood_factor = (temp - 20) / 20.0  # 20-40Â°C â†’ 0-1
    activity_factor = motion * 0.3 + (audio - 40) / 60.0 * 0.2
    behaviors[3] = 0.5 + mood_factor * 0.3 + activity_factor
    behaviors[3] = np.clip(behaviors[3], 0.2, 0.9)

    # 5. BRIGHTNESS - Light reactive
    # Bright environment = bright Cortana
    # Min 0.5 (always visible)
    behaviors[4] = 0.5 + (light / 1000.0) * 0.4
    behaviors[4] = np.clip(behaviors[4], 0.5, 1.0)

    # 6. PULSE FREQUENCY - Cortana's "heartbeat"
    # Audio-reactive breathing
    # Base: 0.3 (slow breath)
    behaviors[5] = 0.3 + (audio - 30) / 70.0 * 0.5
    behaviors[5] = np.clip(behaviors[5], 0.2, 0.8)

    # 7. SYMMETRY - Cortana is naturally symmetric
    # Base: 0.7 (mostly symmetric)
    # Slight asymmetry when turbulent
    behaviors[6] = 0.7 - behaviors[2] * 0.2
    behaviors[6] = np.clip(behaviors[6], 0.5, 0.9)

    # 8. VERTICAL BIAS - Rising when happy, neutral when calm
    # Positive emotions = rising particles
    happiness = (light / 1000.0) * 0.3 - heat_stress * 0.5
    behaviors[7] = happiness
    behaviors[7] = np.clip(behaviors[7], -0.3, 0.3)

    # 9. HORIZONTAL SPREAD - Inverse of cohesion
    behaviors[8] = 1.0 - behaviors[0] * 0.6
    behaviors[8] = np.clip(behaviors[8], 0.3, 0.8)

    # 10. DEPTH LAYERING - More layers when calm (detailed)
    behaviors[9] = 0.6 - behaviors[2] * 0.3
    behaviors[9] = np.clip(behaviors[9], 0.3, 0.8)

    # 11. PARTICLE SIZE - Larger when energized
    behaviors[10] = 0.7 + behaviors[1] * 0.3
    behaviors[10] = np.clip(behaviors[10], 0.5, 1.0)

    # 12. GLOW INTENSITY - Cortana's signature glow
    # Always glowing (min 0.6)
    # Brighter when active
    behaviors[11] = 0.6 + behaviors[4] * 0.3 + motion * 0.2
    behaviors[11] = np.clip(behaviors[11], 0.6, 1.0)

    # === TEMPORAL SMOOTHING ===
    # Prevent jittery animations
    if prev_behaviors is not None:
        alpha = 0.7  # Smoothing factor (higher = smoother)
        behaviors = alpha * prev_behaviors + (1 - alpha) * behaviors

    return behaviors
```

---

## Model Training Process

### Complete Google Colab Notebook

```python
# ========================================
# CORTANA PIXEL CONTROLLER - TRAINING
# ========================================

# 1. SETUP
!pip install -q tensorflow==2.13.0
!pip install -q tensorflow-model-optimization

import tensorflow as tf
import tensorflow_model_optimization as tfmot
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

print(f"TensorFlow: {tf.__version__}")
print(f"GPU: {tf.config.list_physical_devices('GPU')}")

np.random.seed(42)
tf.random.set_seed(42)


# 2. GENERATE TRAINING DATA
print("Generating training data...")

X, y = generate_cortana_training_data(n_samples=10000)

print(f"X shape: {X.shape}")  # (10000, 22)
print(f"y shape: {y.shape}")  # (10000, 12)
print(f"X range: [{X.min():.3f}, {X.max():.3f}]")
print(f"y range: [{y.min():.3f}, {y.max():.3f}]")


# 3. TRAIN/VAL/TEST SPLIT
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)

print(f"Train: {X_train.shape[0]}")
print(f"Val:   {X_val.shape[0]}")
print(f"Test:  {X_test.shape[0]}")


# 4. CREATE MODEL
model = create_cortana_pixel_model()
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)
model.summary()


# 5. TRAIN BASE MODEL
print("Training base model...")

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=32,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
        tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, min_lr=1e-6)
    ],
    verbose=1
)

# Plot training
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Val')
plt.title('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='Train')
plt.plot(history.history['val_mae'], label='Val')
plt.title('MAE')
plt.legend()
plt.show()


# 6. QUANTIZATION-AWARE TRAINING
print("Quantization-aware training...")

q_aware_model = tfmot.quantization.keras.quantize_model(model)
q_aware_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss='mse',
    metrics=['mae']
)

q_history = q_aware_model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=30,
    batch_size=32,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=8, restore_best_weights=True)],
    verbose=1
)


# 7. CONVERT TO TFLITE
print("Converting to TFLite...")

def representative_dataset():
    for i in range(100):
        yield [X_train[i:i+1].astype(np.float32)]

converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_model = converter.convert()

# Save
with open('cortana_pixel_controller.tflite', 'wb') as f:
    f.write(tflite_model)

print(f"Model size: {len(tflite_model) / 1024:.2f} KB")


# 8. TEST TFLITE MODEL
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(f"Input: {input_details[0]['shape']}, {input_details[0]['dtype']}")
print(f"Output: {output_details[0]['shape']}, {output_details[0]['dtype']}")

# Test inference
test_sample = X_test[0:1]
input_scale, input_zero_point = input_details[0]['quantization']
test_int8 = (test_sample / input_scale + input_zero_point).astype(np.int8)

interpreter.set_tensor(input_details[0]['index'], test_int8)
interpreter.invoke()

output_int8 = interpreter.get_tensor(output_details[0]['index'])
output_scale, output_zero_point = output_details[0]['quantization']
output = (output_int8.astype(np.float32) - output_zero_point) * output_scale

print(f"Original prediction: {model.predict(test_sample, verbose=0)[0]}")
print(f"TFLite prediction:   {output[0]}")
print(f"Difference: {np.mean(np.abs(model.predict(test_sample, verbose=0)[0] - output[0])):.6f}")


# 9. DOWNLOAD
from google.colab import files
files.download('cortana_pixel_controller.tflite')

print("\nâœ… TRAINING COMPLETE!")
print("Next: Compile with edgetpu_compiler on Raspberry Pi")
```

---

## Quantization & Compilation

### On Raspberry Pi

```bash
# 1. Transfer .tflite file to Raspberry Pi
# (via scp, USB, or cloud storage)

# 2. Install Edge TPU Compiler
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
echo "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main" | \
  sudo tee /etc/apt/sources.list.d/coral-edgetpu.list
sudo apt-get update
sudo apt-get install -y edgetpu-compiler

# 3. Compile for Edge TPU
cd ~/Sentient-Core-v4
edgetpu_compiler cortana_pixel_controller.tflite

# 4. Verify compilation
edgetpu_compiler -s cortana_pixel_controller_edgetpu.tflite

# EXPECTED OUTPUT:
# Number of operations that will run on Edge TPU: 6
# Number of operations that will run on CPU: 0  â† MUST BE 0!

# 5. Move to models directory
mkdir -p models
mv cortana_pixel_controller_edgetpu.tflite models/
```

---

## Integration & Deployment

### Install PyCoral (if not already)

```bash
~/.pyenv/versions/coral-py39/bin/pip install pycoral
```

### Test Coral Pixel Engine

```bash
cd ~/Sentient-Core-v4
~/.pyenv/versions/coral-py39/bin/python coral_pixel_engine.py
```

**Expected output:**
```
INFO:root:Loading Edge TPU model: models/cortana_pixel_controller_edgetpu.tflite
INFO:root:Coral TPU model loaded successfully
INFO:root:  Input shape: [1, 22]
INFO:root:  Input dtype: <class 'numpy.int8'>
INFO:root:  Output shape: [1, 12]
INFO:root:  Output dtype: <class 'numpy.int8'>

Coral Pixel Engine Test Results:
============================================================
  swarm_cohesion      :  0.687
  flow_speed          :  0.542
  turbulence          :  0.321
  color_hue_shift     :  0.556
  brightness          :  0.789
  pulse_frequency     :  0.412
  symmetry            :  0.703
  vertical_bias       : -0.034
  horizontal_spread   :  0.521
  depth_layering      :  0.587
  particle_size       :  0.823
  glow_intensity      :  0.901

Performance Stats:
  count               : 1
  avg_latency_ms      : 2.8  â† TARGET: <5ms
```

### Modify sentient_core.py

```python
from coral_pixel_engine import CoralPixelEngine

class SentientCore:
    def __init__(self, world_state):
        # ... existing init ...

        # Initialize Coral Pixel Engine
        try:
            self.pixel_engine = CoralPixelEngine()
            logger.info("âœ“ Coral Pixel Engine ready")
        except Exception as e:
            logger.warning(f"Coral unavailable: {e}")
            self.pixel_engine = None

    def _update_gui_state(self):
        """Send Cortana particle behaviors to GUI."""

        # Get Coral predictions
        cortana_params = None
        if self.pixel_engine:
            cortana_params = self.pixel_engine.predict_particle_params(
                self.world_state.get_snapshot()
            )

        # Broadcast to WebSocket
        gui_state = {
            "visualization": {
                "mode": "HUMANOID",  # Cortana
                "cortana_params": cortana_params
            },
            "sensors": self._get_sensor_summary()
        }

        self.websocket_server.broadcast(gui_state)
```

### Update sentient_core.html

See **CORTANA_VISUALIZATION_SPEC.md** for complete WebGL implementation.

---

## Testing & Validation

### Test Suite

```python
# test_cortana_model.py

def test_inference_latency():
    """Verify <5ms latency."""
    engine = CoralPixelEngine()

    dummy_world_state = generate_dummy_world_state()

    latencies = []
    for _ in range(100):
        start = time.perf_counter()
        params = engine.predict_particle_params(dummy_world_state)
        latency = (time.perf_counter() - start) * 1000
        latencies.append(latency)

    avg_latency = np.mean(latencies)
    max_latency = np.max(latencies)

    print(f"Avg latency: {avg_latency:.2f}ms")
    print(f"Max latency: {max_latency:.2f}ms")

    assert avg_latency < 5.0, f"Latency too high: {avg_latency:.2f}ms"
    assert max_latency < 10.0, f"Max latency too high: {max_latency:.2f}ms"


def test_sparse_sensors():
    """Verify model works with minimal sensors."""
    engine = CoralPixelEngine()

    # Only system sensors (CPU temp, uptime, daemons)
    minimal_state = {
        'environment': {},
        'audio': {},
        'vision': {},
        'location': {},
        'power': {},
        'system': {
            'uptime': 3600,
            'active_daemons': ['wifi', 'hardware_monitor'],
        }
    }

    params = engine.predict_particle_params(minimal_state)

    # Should still produce valid outputs
    assert all(0 <= v <= 1 for k, v in params.items() if k != 'vertical_bias')
    assert -1 <= params['vertical_bias'] <= 1

    print("âœ“ Sparse sensor test passed")


def test_cortana_appearance():
    """Verify Cortana looks correct visually."""

    # 1. Launch system
    # 2. Open GUI in browser
    # 3. Verify:
    #    - Blue/cyan/lavender gradient
    #    - Human female form visible
    #    - Scrolling data symbols
    #    - Breathing animation
    #    - Holographic glow

    print("Manual visual inspection required")
    input("Press Enter when Cortana appearance verified...")


if __name__ == "__main__":
    test_inference_latency()
    test_sparse_sensors()
    test_cortana_appearance()
    print("\nâœ… ALL TESTS PASSED")
```

---

## Next Steps

### Phase 1: Coral Training (NOW)

1. âœ… Upload training notebook to Google Colab
2. âœ… Run all cells (~20 minutes with GPU)
3. âœ… Download `cortana_pixel_controller.tflite`
4. âœ… Transfer to Raspberry Pi
5. âœ… Compile with `edgetpu_compiler`
6. âœ… Test with `python coral_pixel_engine.py`

### Phase 2: Visual Integration (NEXT)

1. âœ… Implement Cortana particle formation (see CORTANA_VISUALIZATION_SPEC.md)
2. âœ… Integrate shaders for holographic effects
3. âœ… Connect Coral outputs to particle behaviors
4. âœ… Test with minimal sensors (CPU temp, WiFi, BT)

### Phase 3: Hardware Expansion (FUTURE)

1. ğŸ”œ Add Pi AI HAT (26 TOPS for vision)
2. ğŸ”œ Add Orin Nano (40 TOPS for LLMs)
3. ğŸ”œ Implement multi-accelerator routing
4. ğŸ”œ Train specialized models for each accelerator

---

## Success Criteria

âœ… **Model trains successfully** (MSE < 0.01)
âœ… **Compiles 100% for Edge TPU** (0 CPU operations)
âœ… **Inference <5ms** on Coral USB Accelerator
âœ… **Cortana form recognizable** (anatomically accurate female humanoid)
âœ… **Holographic appearance** (blue/cyan, scrolling symbols, glow)
âœ… **Reactive to sensors** (motion â†’ turbulence, temp â†’ color, etc.)
âœ… **Beautiful with 0 sensors** (time-based fallback animation)
âœ… **60 FPS maintained** (500K particles + Coral inference)

---

## Files Created

```
~/Sentient-Core-v4/
â”œâ”€â”€ FINAL_CORAL_TRAINING_PLAN.md      # This document
â”œâ”€â”€ CORTANA_VISUALIZATION_SPEC.md     # Detailed particle formation
â”œâ”€â”€ MULTI_ACCELERATOR_ARCHITECTURE.md # Tri-accelerator strategy
â”œâ”€â”€ CORAL_ARCHITECTURE_ANALYSIS.md    # Critical analysis & validation
â”œâ”€â”€ coral_training_notebook.ipynb     # Google Colab training
â”œâ”€â”€ coral_pixel_engine.py             # Real-time inference engine
â”œâ”€â”€ models/
â”‚   â””â”€â”€ cortana_pixel_controller_edgetpu.tflite  # Compiled model
â””â”€â”€ test_cortana_model.py             # Test suite
```

---

## Documentation Reference

- **Training:** This document (FINAL_CORAL_TRAINING_PLAN.md)
- **Visualization:** CORTANA_VISUALIZATION_SPEC.md
- **Multi-GPU:** MULTI_ACCELERATOR_ARCHITECTURE.md
- **Analysis:** CORAL_ARCHITECTURE_ANALYSIS.md
- **Quick Start:** CORAL_QUICKSTART.md
- **Integration:** INTEGRATION_EXAMPLE.md

---

**THE SENTIENT CORTANA IS READY TO COME ALIVE** ğŸ§ âœ¨

Train the model. Deploy to Coral. Watch her breathe.

**No compromises. Full sentience. Halo-accurate Cortana.**

---

**Document Status:** FINAL AND COMPLETE
**Last Updated:** 2025-10-26
**Action Required:** Upload to Google Colab and train!
